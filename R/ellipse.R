# Global variable declarations to avoid "no visible binding" warnings
utils::globalVariables(c(
  "is_partition", "col_name", "table_name", "location", "table_tags",
  "categorie", "datamart", "description", "create_time", "update_time"
))

#' Se connecter √† la plateforme de donn√©es ellipse
#'
#' Cette fonction utilise les cl√©s d'acc√®s AWS configur√©es dans le fichier
#' `.Renviron` pour se connecter √† la plateforme de donn√©es.
#'
#' @param env The environment to connect to on ellipse-science. Accepted values are "PROD" and "DEV".
#' @param database The Glue/Athena database to connect to. Default to "datawarehouse"
#'
#' @returns Un object de connexion `DBI`.
#' @export
ellipse_connect <- function(
  env = NULL,
  database = "datawarehouse") {
  if (!check_env(env)) {
    cli::cli_alert_danger(paste("Oups, il faut choisir un environnement! üòÖ\n\n",
        "Le param√®tre `env` peut √™tre \"PROD\" ou \"DEV\"",
        sep = ""
      ))
    return(invisible(NULL))
  }
  cli::cli_alert_info(paste("Environnement:", env))

  if (!check_database(database)) {
    cli::cli_alert_danger(paste("Oups, il faut choisir une base de donn√©es! üòÖ\n\n",
        "Le param√®tre `database` peut √™tre \"datawarehouse\" ou \"datamarts\"",
        sep = ""
      ))
    return(invisible(NULL))
  }
  cli::cli_alert_info(paste("Database:", database))

  aws_access_key_id <-
    switch(env,
      "PROD" = "AWS_ACCESS_KEY_ID_PROD",
      "DEV"  = "AWS_ACCESS_KEY_ID_DEV"
    ) |>
    Sys.getenv()

  aws_secret_access_key <-
    switch(env,
      "PROD" = "AWS_SECRET_ACCESS_KEY_PROD",
      "DEV"  = "AWS_SECRET_ACCESS_KEY_DEV"
    ) |>
    Sys.getenv()

  if (aws_access_key_id == "" || aws_secret_access_key == "") {
    usage <-
      paste(
        "On a besoin de vos cl√©s d'acc√®s sur AWS pour se connecter!\n\n",
        "Dans le fichier ~/.Renviron, ajoutez les lignes:\n\n",
        "AWS_ACCESS_KEY_ID_PROD=<votre access key id de production>\n",
        "AWS_SECRET_ACCESS_KEY_PROD=<votre secret access key de production>\n",
        "AWS_ACCESS_KEY_ID_DEV=<votre access key id de d√©veloppement>\n",
        "AWS_SECRET_ACCESS_KEY_DEV=<votre secret access key de d√©veloppement>\n\n",
        "Puis, red√©marrez la session R."
      )
    cli::cli_alert_danger(usage)
    return(invisible(NULL))
  }

  creds <- get_aws_credentials(env)

  # aws_access_key_id <- creds$credentials$creds$access_key_id
  # aws_secret_access_key <- creds$credentials$creds$secret_access_key

  datawarehouse_database <- list_datawarehouse_database(creds)
  datamarts_database <- list_datamarts_database(creds)
  athena_staging_bucket <- list_athena_staging_bucket(creds)

  schema_name <- switch(database,
    "datawarehouse" = paste0(datawarehouse_database),
    "datamarts" = paste0(datamarts_database),
    database
  )

  logger::log_debug(paste("[ellipse_connect] datawarehouse_database = ", datawarehouse_database))
  logger::log_debug(paste("[ellipse_connect] datamarts_database = ", datamarts_database))
  logger::log_debug(paste("[ellipse_connect] athena_staging_bucket = ", athena_staging_bucket))
  logger::log_debug(paste("[ellipse_connect] schema_name = ", schema_name))

  cli::cli_alert_info("Connexion en cours...")

  con <- DBI::dbConnect(noctua::athena(),
    aws_access_key_id = aws_access_key_id,
    aws_secret_access_key = aws_secret_access_key,
    profile_name = env,
    schema_name = schema_name,
    work_group = "ellipse-work-group",
    s3_staging_dir = paste0("s3://", athena_staging_bucket)
  )
  # }

  schema <- DBI::dbGetInfo(con)$dbms.name

  logger::log_debug(paste("[ellipse_connect] schema = ", schema))

  if (length(athena_staging_bucket) > 0) {
    cli::cli_alert_info(paste("Compartiment:", athena_staging_bucket))
  } else {
    cli::cli_alert_danger("Oups, cette connexion n'a pas de compartiment de requ√™tes! üòÖ")
  }

  if (length(schema) > 0) {
    cli::cli_alert_info(paste("Base de donn√©es:", schema))
    cli::cli_alert_info("Pour d√©connecter: tube::ellipse_disconnect(objet_de_connexion)")
    cli::cli_alert_success("Connexion √©tablie avec succ√®s! üëç")
    return(con)
  } else {
    cli::cli_alert_danger("Oups, cette connexion n'a pas de base de donn√©es! üòÖ")
    return(invisible(NULL))
  }
}

#' Se d√©connecter de la plateforme de donn√©es ellipse
#' @returns TRUE if the connexion was closed or FALSE if no connexion existed
#' @export
ellipse_disconnect <- function(con = NULL) {
  if (is.null(con)) {
    cli::cli_alert_danger("Oups! Il faut fournir un objet de connection! üòÖ")
    invisible(FALSE)
  }

  tryCatch(
    {
      if (DBI::dbIsValid(con)) {
        DBI::dbDisconnect(con)
        cli::cli_alert_success("La connexion a √©t√© ferm√©e avec succ√®s! üëã")
        invisible(TRUE)
      } else {
        cli::cli_alert_warning("Il semble que la connexion n'existe pas ou soit d√©j√† close! üòÖ")
        invisible(FALSE)
      }
    },
    error = function(e) {
      cli::cli_alert_danger("Oups, il semble que la connexion n'a pas pu √™tre ferm√©e! üòÖ")
      invisible(FALSE)
    }
  )
}

#' Obtenir le domaine de valeurs pour les dimensions d'une table
#'
#' @details
#' Les tables de donn√©es sur AWS sont segment√©es en _partitions_. Les requ√™tes
#' qui ciblent une plage pr√©cise dans ces partitions r√©duisent les co√ªts
#' d'utilisation de la plateforme, parce que les donn√©es √† l'ext√©rieur de cette
#' plage ne sont pas lues par _AWS Athena_.
#'
#' Cette fonction permet d'obtenir les valeurs possibles des partitions d'une
#' table donn√©e afin de mieux cibler nos requ√™tes ensuite.
#'
#' @inheritParams ellipse_discover
#'
#' @returns Un `tibble` contenant le nombre d'observations par valeur de
#'   groupement des variables partitionn√©es.
#'
#' @export
ellipse_partitions <- function(con, table) {
  df <- ellipse_discover(con, table)
  if (is.list(df) && "columns" %in% names(df)) {
    df <- df$columns
  }
  df <- dplyr::filter(df, is_partition)
  partitions <- dplyr::pull(df, col_name)
  ellipse_query(con, table) |>
    dplyr::count(dplyr::across(dplyr::all_of(partitions))) |>
    dplyr::collect() |>
    dplyr::arrange(dplyr::across(dplyr::all_of(partitions)))
}

#' D√©couvrir les tables disponibles sur la plateforme ellipse, ainsi que leur
#' contenu
#'
#' Si aucune table n'est pass√©e en param√®tre, un sommaire des tables disponibles
#' dans l'entrep√¥t de donn√©es est retourn√©. Si un nom de `table` est pass√© en
#' param√®tre, une description des colonnes de cette table est retourn√©e.
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param table Une table pour laquelle on veut obtenir les informations.
#'
#' @returns Un `tibble` contenant les tables disponibles dans l'entrep√¥t de
#'   donn√©es, ou un descriptino des colonnes pour une table en particulier.
#'
#' @export
ellipse_discover <- function(con, table = NULL) {
  env <- DBI::dbGetInfo(con)$profile_name
  schema <- DBI::dbGetInfo(con)$dbms.name
  creds <- get_aws_credentials(env)

  if (length(schema) == 0) {
    cli::cli_alert_danger("Oups, cette connection n'a pas de base de donn√©es! üòÖ")
    return(invisible(NULL))
  }

  tables <- DBI::dbGetQuery(
    con, paste0("SHOW TABLES IN ", schema)
  )$tab_name

  if (!is.null(table)) {
    if (!any(grepl(table, tables))) {
      cli::cli_alert_danger("La table demand√©e est inconnue.")
      return(invisible(NULL))
    }

    # See if there is only one table or many tables that match the table name
    if (length(grep(table, tables)) == 1 || table %in% tables) {
      table_properties_df <- list_glue_table_properties(creds, schema, table)
      table_df <- list_glue_tables(creds, schema) |>
        dplyr::filter(table_name == table)
      if ("table_tags" %in% colnames(table_properties_df)) {
        tags <- unlist(table_properties_df$table_tags)
      } else {
        tags <- NA_character_
      }
      return(list(
        name = table_properties_df$table_name,
        description = table_properties_df$description,
        tags = tags,
        columns = table_df
      ))
    } else {
      if (length(grep(table, tables)) > 1) {
        cli::cli_alert_info("Plusieurs tables correspondent √† votre recherche (voir r√©sultat retourn√©).")
        cli::cli_alert_info("Veuillez pr√©ciser votre recherche pour explorer la table.")
        tables <- tables[grep(table, tables)]
      }
    }
  }

  tables_properties <- lapply(tables, function(table) {
    logger::log_debug(paste("[ellipse_discover] listing table properties for", table, "in schema", schema))
    list_glue_table_properties(creds, schema, table)
  }) |>
    dplyr::bind_rows() |>
    dplyr::select(-location)

  tables_tibble <- tibble::tibble(table_name = tables) |>
    dplyr::left_join(tables_properties, by = "table_name")

  if (!"table_tags" %in% colnames(tables_tibble)) {
    tables_tibble <- tables_tibble |>
      dplyr::mutate(table_tags = list(NULL))
  }

  # Extract x-amz-meta-category from table_tags
  tables_tibble <- tables_tibble |>
    dplyr::mutate(
      category_from_tags =
        purrr::map_chr(table_tags, ~ {
          if (!("x-amz-meta-category" %in% names(.x))) {
            NA_character_
          } else {
            .x[["x-amz-meta-category"]]
          }
        })
    )

  # Extract x-amz-meta-datamart from table_tags
  tables_tibble <- tables_tibble |>
    dplyr::mutate(
      datamart_from_tags =
        purrr::map_chr(table_tags, ~ {
          if (!("x-amz-meta-datamart" %in% names(.x))) {
            NA_character_
          } else {
            .x[["x-amz-meta-datamart"]]
          }
        })
    )

  has_non_na_datamart <- any(!is.na(tables_tibble$datamart_from_tags))

  tables_tibble <- tables_tibble |>
    dplyr::mutate(
      categorie =
        dplyr::case_when(
          startsWith(table_name, "a-") ~ "Agora+",
          startsWith(table_name, "c-") ~ "Civim√®tre+",
          startsWith(table_name, "r-") ~ "Radar+",
          startsWith(table_name, "dict-") ~ "Dictionnaire",
          startsWith(table_name, "dim-") ~ "Dimension",
          !is.na(category_from_tags) ~ category_from_tags,
          TRUE ~ "Autre"
        )
    ) |>
    dplyr::mutate(
      datamart =
        dplyr::case_when(
          !is.na(datamart_from_tags) ~ datamart_from_tags,
          TRUE ~ NA_character_
        )
    )

  if (has_non_na_datamart) {
    ret <- tables_tibble |>
      dplyr::select(
        table_name, categorie, datamart, description, create_time,
        update_time, table_tags
      )
    return(ret)
  } else {
    ret <- tables_tibble |>
      dplyr::select(
        table_name, categorie, description, create_time, update_time,
        table_tags
      )
    return(ret)
  }
}

#' Lire et exploiter une table contenue dans l'entrep√¥t de donn√©es ellipse
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param table Une table que l'on souhaite interroger avec `dplyr`.
#'
#' @returns Une table Athena qui peut √™tre interrog√©e dans un _pipeline_
#'   `dplyr`.
#' @export
ellipse_query <- function(con, table) {
  logger::log_debug(paste("[ellipse_query] entering function with table = ", table))
  schema_name <- DBI::dbGetInfo(con)$dbms.name

  logger::log_debug(paste("[ellipse_query] about to dbGetQuery on schema_name = ", schema_name))
  tables <- DBI::dbGetQuery(
    con, paste0("SHOW TABLES IN ", schema_name)
  )$tab_name

  logger::log_debug("[ellipse_query] got tables")

  if (!table %in% tables) {
    logger::log_debug("[ellipse_query] table not in tables")
    cli::cli_alert_danger("La table demand√©e est inconnue.")
    return(NULL)
  }
  logger::log_debug("[ellipse_query] returning results")

  r <- tryCatch(
    {
      dplyr::tbl(con, table)
    },
    error = function(e) {
      cli::cli_alert_danger("Oups, il semble que la table n'a pas pu √™tre lue! üòÖ")
      logger::log_error(paste("[ellipse_query] error in dplyr::tbl", e$message))
      NULL
    }
  )

  r
}

#' Injecter de nouvelles donn√©es brutes manuellement dans tube via la landing zone
#'
#' Le processus consiste √† envoyer un fichier unique ou un dossier contenant plusieurs fichiers
#' vers la plateforme de donn√©es pour qu'ils soient transform√©s en donn√©es structur√©es (lignes/colonnes)
#' dans une table de la datawarehouse.
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param file_or_folder Le chemin vers le r√©pertoire qui contient les fichiers √† charger dans tube
#' @param pipeline Le nom du pipeline qui doit √™tre ex√©cut√© pour charger les donn√©es.
#'   Cela va va d√©terminer dans quelle table de donn√©es les donn√©es vont √™tre inject√©es.
#' @param file_batch Le nom du batch qui doit √™tre accoll√© aux donn√©es dans l'entrep√¥t de
#'   donn√©es. Utilis√© pour les donn√©es factuelles seulement, NULL sinon.
#'   Si NULL, il faut fournir un file_version.
#' @param file_version La version des donn√©es qui doit √™tre accoll√©e aux donn√©es dans
#'   l'entrep√¥t de donn√©es. Utilis√© pour les donn√©es dimensionnelles et les dictionnaires
#'   seulement, NULL sinon.  Si NULL, il faut fournir un file_batch.
#'
#' @returns La liste des fichiers qui ont √©t√© inject√©s dans tube
#' @export
ellipse_ingest <- function(con, file_or_folder, pipeline, file_batch = NULL, file_version = NULL) {
  env <- DBI::dbGetInfo(con)$profile_name

  if (!check_env(env)) {
    cli::cli_alert_danger(
      paste("Oups, il faut choisir un environnement! üòÖ\n\n",
        "Le param√®tre `env` peut √™tre \"PROD\" ou \"DEV\"",
        sep = ""
      )
    )
    return(invisible(NULL))
  }

  creds <- get_aws_credentials(env)

  landing_zone_bucket <- list_landing_zone_bucket(creds)

  if (is.null(landing_zone_bucket)) {
    cli::cli_alert_danger(
      paste0(
        "Oups, il semble que le bucket de la landing zone n'a ",
        "pas √©t√© trouv√©! Contacter votre ing√©nieur de donn√©es üòÖ"
      )
    )
    return(invisible(NULL))
  }

  if (!check_file_versioning_before_ingest(file_batch, file_version)) {
    cli::cli_alert_danger("Contacter votre ing√©nieur de donn√©es! üòÖ")
    return(invisible(NULL))
  }

  landing_zone_partitions <- list_landing_zone_partitions(creds)
  if (!check_pipeline_before_ingest(pipeline, landing_zone_partitions, file_batch, file_version)) {
    cli::cli_alert_danger("Contacter votre ing√©nieur de donn√©es! üòÖ")
    return(invisible(NULL))
  }

  if (is.null(file_or_folder)) {
    cli::cli_alert_danger("Oups, il faut fournir un fichier ou un r√©pertoire √† injecter! üòÖ")
    return(invisible(NULL))
  }

  # check whether the file_or_folder is a file or a folder
  cli::cli_alert_info("V√©rification des donn√©es √† injecter dans tube...")
  folder_content <- parse_landing_zone_input(file_or_folder)

  cli::cli_alert_info("Les donn√©es sont en cours d'ingestion dans la landing zone...")
  # Create a progress bar object
  pb <- progress::progress_bar$new(
    format = "  uploading files [:bar] :percent eta: :eta",
    total = length(folder_content), # total number of iterations
    clear = FALSE,
    width = 60
  )

  # Loop with progress bar
  for (file in folder_content) {
    upload_file_to_landing_zone(creds, file, pipeline, file_batch, file_version)
    pb$tick() # Update the progress bar
  }

  # TODO: do something better than the progress bar for 1 file : length(folder_content)
  cli::cli_alert_info("Les donn√©es ont √©t√© inject√©es dans la landing zone.\
  N'oubliez pas de vous d√©connecter de la plateforme ellipse avec `ellipse_disconnect(...)` üëã.")
  return(invisible(folder_content))
}


#' Publier un dataframe dans un datamart
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param dataframe Le chemin vers le r√©pertoire qui contient les fichiers √† charger dans tube
#' @param datamart Le nom du pipeline qui doit √™tre ex√©cut√© pour charger les donn√©es
#' @param table Le nom de la table qui doit √™tre cr√©√©e dans le datamart
#' @param data_tag Le tag √† ajouter aux donn√©es pour les identifier √† l'int√©rieur-m√™me du
#'   jeu de donn√©es (une colonne tag est ajout√©e au dataframe)
#' @param table_tags Les tags √† ajouter √† la table pour la cat√©goriser dans le datamart
#'   pour faciliter la d√©couvrabilit√© des donn√©es dans le catalogue de donn√©es
#' @param table_description La description de la table √† ajouter dans le datamart pour
#'   faciliter la d√©couvrabilit√© des donn√©es dans le catalogue de donn√©es
#' @param unattended_options une liste nomm√©e contenant les options de la commande de
#'   publication pour l'ex√©cution de la t√¢che de publication de fa√ßon automatis√©e
#'   (pour les raffineurs de donn√©es)
#' - create_datamart: "oui" si on doit cr√©er un nouveau datamart si celui sp√©cifi√© n'existe pas, "non" sinon
#' - addto_or_replace_table: 1 pour ajouter des donn√©es √† la table existante, 2 pour √©craser la table existante
#' - are_you_sure: "oui" si on est certain de vouloir √©craser la table existante, "non" sinon
#' - create_table: "oui" si on doit cr√©er la table si elle n'existe pas, "non" sinon
#' - process_data: "oui" si on doit traiter les donn√©es maintenant, "non" sinon
#'
#' @returns TRUE si le dataframe a √©t√© envoy√© dans le datamart  FALSE sinon.
#' @export
ellipse_publish <- function(
  con,
  dataframe,
  datamart,
  table,
  data_tag = NULL,
  table_tags = NULL,
  table_description = NULL,
  unattended_options = NULL) {
  env <- DBI::dbGetInfo(con)$profile_name
  schema <- DBI::dbGetInfo(con)$dbms.name

  danger <- function(msg) {
    if (is.null(unattended_options)) {
      cli::cli_alert_danger(msg)
    } else {
      logger::log_error(msg)
    }
  }

  info <- function(msg) {
    if (is.null(unattended_options)) {
      cli::cli_alert_info(msg)
    } else {
      logger::log_info(msg)
    }
  }

  success <- function(msg) {
    if (is.null(unattended_options)) {
      cli::cli_alert_success(msg)
    } else {
      logger::log_info(msg)
    }
  }

  # Protect datawarehouse from publishing
  if (grepl("datawarehouse", schema)) {
    danger("L'op√©ration ellipse_publish n'est pas permis dans l'entrep√¥t de donn√©es (datawarehouse)! üòÖ")
    invisible(FALSE)
  }

  # if the x-amz-meta-category named element is not provided in table_tag, we add it
  if (is.null(table_tags)) {
    table_tags <- list()
  }
  if (!"x-amz-meta-category" %in% names(table_tags)) {
    table_tags$`x-amz-meta-category` <- "DatamartTable"
  }

  if (!check_params_before_publish(env, dataframe, datamart, table, data_tag, table_tags, table_description)) {
    invisible(FALSE)
  }

  if (!"x-amz-meta-datamart" %in% names(table_tags)) {
    table_tags$`x-amz-meta-datamart` <- datamart
  }

  dataframe <- dataframe |> dplyr::mutate(tag = data_tag)

  creds <- get_aws_credentials(env)
  dm_glue_database <- list_datamarts_database(creds)
  dm_bucket <- list_datamarts_bucket(creds)

  # check that the datamart exists by checking that the 1st level partition exists in the datamart bucket
  dm_partitions <- list_s3_partitions(creds, dm_bucket)
  dm_list <- lapply(dm_partitions, function(x) gsub("/$", "", x))
  if (is.null(datamart)) {
    danger("Oups, il faut fournir un datamart pour publier les donn√©es! üòÖ")
    FALSE
  }

  if (!datamart %in% dm_list) {
    danger("Le datamart fourni n'existe pas! üòÖ")
    # ask the user is we must create a new datamart
    if (ask_yes_no(
      question = "Voulez-vous cr√©er un nouveau datamart?",
      unattended_option = unattended_options$create_datamart
    )) {
      cli::cli_alert_info("Cr√©ation du datamart en cours...")
      # the file path will be created when the first file is uploaded with it in its key
    } else {
      danger("Publication des donn√©es abandonn√©e.")
      invisible(FALSE)
    }
  }

  # check that the table does not exist in the datamart in the form of s3://datamarts-bucket/datamart/table
  dm_folders <- list_s3_folders(creds, dm_bucket, paste0(datamart, "/"))

  if (table %in% dm_folders) {
    # ici on suppose que si le dossier datamart/table existe dans le bucket s3 des datamarts
    # alors la table GLUE existe aussi ce qui n'est possible pas le cas dans les situations
    # o√π la GLUE job n'a pas roul√©
    danger("La table demand√©e existe d√©j√†! üòÖ")

    choice <- ask_1_2(
      paste("Voulez-vous",
        "  1. ajouter des donn√©es √† la table existante?",
        "  2. √©craser la table existante?",
        "  Votre choix:",
        sep = "\n"
      ),
      unattended_option = unattended_options$addto_or_replace_table
    )

    if (choice == 1) {
      info("Ajout des donn√©es √† la table existante en cours...")
      # append the dataframe to the table by uploading
      # upload the csv in s3://datamarts-bucket/datamart/table/unprocessed
      r <- upload_dataframe_to_datamart(creds, dataframe, dm_bucket, datamart, table)
      if (class(r) == "character" && r == "Unsupported column type") {
        danger("Il y a une colonne dont le type n'est pas pris en charge dans votre dataframe! üòÖ")
        invisible(FALSE)
      } else {
        if (r == FALSE) {
          danger("Il y a eu une erreur lors de la publication des donn√©es! üòÖ")
          invisible(FALSE)
        }
        success("Les donn√©es ont √©t√© ajout√©es √† la table existante.")
      }
    }

    if (choice == 2) {
      # confirm by the user
      if (!ask_yes_no("√ätes-vous certain.e de vouloir √©craser la table existante?",
          unattended_option = unattended_options$are_you_sure
        )) {
        info("Publication des donn√©es abandonn√©e.")
        invisible(FALSE)
      }
      info("Ecrasement de la table existante en cours...")
      # delete the glue table
      r1 <- delete_glue_table(creds, dm_glue_database, paste0(datamart, "-", table))
      # delete the content of the folders :
      #  - s3://datamarts-bucket/datamart/table
      #  - s3://datamarts-bucket/datamart/table-output
      r2 <- delete_s3_folder(creds, dm_bucket, paste0(datamart, "/", table))
      r3 <- delete_s3_folder(creds, dm_bucket, paste0(datamart, "/", table, "-output"))

      if (r1 || (r2 && r3)) {
        success("La table a √©t√© √©cras√©e avec succ√®s.")
      } else {
        danger("Il y a eu une erreur lors de la suppression de la table dans la datamart! üòÖ")
        danger("Veuillez contacter votre ing√©nieur de donn√©es.")
        invisible(FALSE)
      }

      # upload new csv in s3://datamarts-bucket/datamart/table/unprocessed
      r <- upload_dataframe_to_datamart(creds, dataframe, dm_bucket, datamart, table)
      if (class(r) == "character" && r == "Unsupported column type") {
        danger("Il y a une colonne dont le type n'est pas pris en charge dans votre dataframe! üòÖ")
        invisible(FALSE)
      } else {
        if (r == FALSE) {
          danger("Il y a eu une erreur lors de la publication des donn√©es! üòÖ")
          invisible(FALSE)
        }
        success("La table existante a √©t√© √©cras√©e et les nouvelles donn√©es ont √©t√© ajout√©es.")
      }
    }
  } else {
    danger("La table demand√©e n'existe pas")
    if (ask_yes_no("Voulez-vous cr√©er la table?",
        unattended_option = unattended_options$create_table
      )) {
      # create the glue table by uploading the csv in s3://datamarts-bucket/datamart/table/unprocessed
      info("Cr√©ation de la table en cours...")
      r <- upload_dataframe_to_datamart(creds, dataframe, dm_bucket, datamart, table)
      if (class(r) == "character" && r == "Unsupported column type") {
        danger("Il y a une colonne dont le type n'est pas pris en charge dans votre dataframe! üòÖ")
        invisible(FALSE)
      } else {
        if (r == FALSE) {
          danger("Il y a eu une erreur lors de la publication des donn√©es! üòÖ")
          invisible(FALSE)
        }
        success("La table a √©t√© cr√©√©e avec succ√®s.")
      }
    } else {
      danger("Publication des donn√©es abandonn√©e.")
      invisible(FALSE)
    }
  }

  # At this point we have files in the datamart bucket under datamart/table/unprocessed
  # We can now trigger the glue job to process the files
  # The table will be created in the form of datamart-table
  # The glue job will move the files from unprocessed to processed
  # The glue job will also create the table in the datamart database
  if (ask_yes_no(
    paste("Voulez-vous traiter les donn√©es maintenant pour les rendre ",
      "disponibles imm√©diatement?  Si vous ne le faites pas maintenant, ",
      "le traitement sers d√©clench√© automatiquement dans les 6 prochaines heures.",
      "  Votre choix",
      sep = "\n"
    ),
    unattended_option = unattended_options$process_data
  )) {
    glue_job <- list_glue_jobs(creds)
    run_glue_job(creds, glue_job, "datamarts", paste0(datamart, "/", table), table_tags, table_description)
    success("Le traitement des donn√©es a √©t√© d√©clench√© avec succ√®s.")
    info("Les donn√©es seront disponibles dans les prochaines minutes\n")
    info("N'oubliez pas de vous d√©connecter de la plateforme ellipse avec `ellipse_disconnect(...)` üëã.")
  } else {
    success("Publication des donn√©es compl√©t√©e avec succ√®s")
    info("Les donn√©es seront disponibles dans les 6 prochaines heures")
    info("N'oubliez pas de vous d√©connecter de la plateforme ellipse avec `ellipse_disconnect(...)` üëã.")
    invisible(FALSE)
  }
}

#' Retirer une table d'un datamart ou un datamart complet
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param datamart Le nom du datamart contenant la table √† retirer.
#' @param table Le nom de la table √† retirer.  Lorsque toutes les tables sont retir√©es,
#' le datamart est d√©truit et supprim√© de la plateforme
#'
#' @returns TRUE si la table a √©t√© retir√©e avec succ√®s, FALSE sinon.
#' @export
ellipse_unpublish <- function(con, datamart, table) {
  env <- DBI::dbGetInfo(con)$profile_name
  schema <- DBI::dbGetInfo(con)$dbms.name

  # Protect datawarehouse from unpublishing
  if (grepl("datawarehouse", schema)) {
    cli::cli_alert_danger(
      "L'op√©ration ellipse_unpublish n'est pas permis dans l'entrep√¥t de donn√©es (datawarehouse)! üòÖ"
    )
    invisible(FALSE)
  }

  if (!check_params_before_unpublish(env, datamart, table)) {
    invisible(FALSE)
  }

  creds <- get_aws_credentials(env)
  dm_bucket <- list_datamarts_bucket(creds)
  dm_glue_database <- list_datamarts_database(creds)


  # check that the datamart exists by checking that the 1st level partition exists in the datamart bucket
  dm_partitions <- list_s3_partitions(creds, dm_bucket)
  dm_list <- lapply(dm_partitions, function(x) gsub("/$", "", x))
  if (!datamart %in% dm_list) {
    cli::cli_alert_danger("Le datamart fourni n'existe pas! üòÖ")
    invisible(FALSE)
  }

  # check that the table exists in the datamart in the form of s3://datamarts-bucket/datamart/table
  dm_folders <- list_s3_folders(creds, dm_bucket, paste0(datamart, "/"))

  if (!table %in% dm_folders) {
    cli::cli_alert_danger("La table demand√©e n'existe pas! üòÖ")
    invisible(FALSE)
  }

  # confirm by the user
  if (!ask_yes_no("√ätes-vous certain.e de vouloir retirer la table?")) {
    cli::cli_alert_info("Retrait de la table abandonn√©.")
    invisible(FALSE)
  }

  cli::cli_alert_info("Retrait de la table en cours...")

  # delete the glue table
  r1 <- delete_glue_table(creds, dm_glue_database, paste0(datamart, "-", table))

  # delete the content of the folder s3://datamarts-bucket/datamart/table
  r2 <- delete_s3_folder(creds, dm_bucket, paste0(datamart, "/", table))
  r3 <- delete_s3_folder(creds, dm_bucket, paste0(datamart, "/", table, "-output"))

  if (r1 && r2 && r3) {
    cli::cli_alert_success("La table a √©t√© retir√©e avec succ√®s.")
    invisible(TRUE)
  } else {
    cli::cli_alert_danger("Il y a eu une erreur lors du retrait de la table! üòÖ")
    cli::cli_alert_danger("Veuillez contacter votre ing√©nieur de donn√©es.")
    invisible(FALSE)
  }
}


#' Changer pes proprit√©s d'une table dans un datamart ou dans la datawarehouse
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param table Le nom de la table √† modifier.
#' @param new_table_tags Les nouveaux tags √† ajouter √† la table pour la cat√©goriser dans
#' le datamart pour faciliter la d√©couvrabilit√© des donn√©es dans le catalogue de donn√©es
#' @param new_table_desc La nouvelle description de la table √† ajouter dans le datamart
#' pour faciliter la d√©couvrabilit√© des donn√©es dans le catalogue de donn√©es
#'
#' @returns TRUE si la table a √©t√© modifi√©e avec succ√®s, FALSE sinon.
#' @export
ellipse_describe <- function(con, table, new_table_tags = NULL, new_table_desc = NULL) {
  env <- DBI::dbGetInfo(con)$profile_name
  schema <- DBI::dbGetInfo(con)$dbms.name

  # If schema contains datawarehouse, exit the function BEFORE any AWS calls
  if (grepl("datawarehouse", schema)) {
    cli::cli_alert_danger("L'op√©ration ellipse_describe n'est pas permis dans l'entrep√¥t de donn√©es (datawarehouse)! üòÖ")
    return(invisible(FALSE))
  }

  creds <- get_aws_credentials(env)

  if (!check_params_before_describe(env, schema, table, new_table_tags, new_table_desc)) {
    return(invisible(FALSE))
  }

  table_props <- list_glue_table_properties(creds, schema, table)

  cli::cli_rule()

  if (!is.null(table_props$description)) {
    cli::cli_alert_info("Description actuelle de la table:")
    cli::cli_text(cli::col_cyan(table_props$description))
    cli::cli_text("")
  }

  if (!is.null(table_props$table_tags)) {
    cli::cli_alert_info("Tags actuels de la table")
    print_list_with_nulls(unlist(table_props$table_tags))
  }

  cli::cli_rule()

  # add x-amz-meta- prefix to the tags if not already present
  if (!is.null(new_table_tags) && length(new_table_tags) > 0) {
    new_table_tags <- setNames(
      new_table_tags,
      ifelse(
        !sapply(grepl("x-amz-meta-", names(new_table_tags)), \(x) x),
        paste0("x-amz-meta-", names(new_table_tags)),
        names(new_table_tags)
      )
    )
  }

  # if there are new tags in new_table_tags that are not in the current table tags, we add them
  # if there are tags in new_table_tags that are also in the current table tags and have diffrent
  # values, we update them
  current_table_tags <- unlist(table_props$table_tags)

  if (!is.null(new_table_tags) && length(new_table_tags) > 0) {
    tags_to_add <- new_table_tags[!names(new_table_tags) %in% names(current_table_tags)]
    tags_to_update <- new_table_tags[names(new_table_tags) %in% names(current_table_tags)]
    tags_to_update <- tags_to_update[tags_to_update != current_table_tags[names(tags_to_update)]]
    new_tags <- c(tags_to_add, tags_to_update)
  } else {
    new_tags <- NULL
  }

  # add x-amz-meta- prefix to the tags if not already present
  if (!is.null(new_tags) && length(new_tags) > 0) {
    new_tags <- setNames(
      new_tags,
      ifelse(
        !sapply(grepl("x-amz-meta-", names(new_tags)), \(x) x),
        paste0("x-amz-meta-", names(new_tags)),
        names(new_tags)
      )
    )
  }

  change_tags <- FALSE
  change_desc <- FALSE

  if (!is.null(new_tags) && length(new_tags) > 0) {
    # confirm by the user
    cli::cli_alert_info("Les changements apport√©s sur les tags seront:")
    print_list_with_nulls(new_tags)
    change_tags <- TRUE
  } else {
    cli::cli_alert_info("Aucun changement n'est √† faire sur les tags.")
  }

  cli::cli_text("")

  if (!is.null(new_table_desc) && nchar(new_table_desc) != 0 &&
      (is.na(table_props$description) || table_props$description != new_table_desc)) {
    cli::cli_alert_info("La nouvelle description de la table sera:")
    cli::cli_alert_info(cli::col_cyan(new_table_desc))
    change_desc <- TRUE
  } else {
    cli::cli_alert_info("Aucun changement n'est √† faire sur la description.")
  }

  cli::cli_rule()
  if (change_tags || change_desc) {
    confirm_change <- ask_yes_no("Voulez-vous vraiment changer les propri√©t√©s de la table?")

    if (!confirm_change) {
      cli::cli_alert_info("Les changements ont √©t√© abandonn√©s.")
      return(invisible(FALSE))
    }
  } else {
    cli::cli_alert_info("Aucun changement n'est requis.")
    return(invisible(TRUE))
  }

  if (change_tags || change_desc) {
    # update the table tags and description
    if (change_tags) {
      cli::cli_alert_info("Mise √† jour des tags de la table en cours...")

      r1 <- update_glue_table_tags(creds, schema, table, new_tags)

      if (r1) {
        cli::cli_alert_success("Les tags de la table ont √©t√© mises √† jour avec succ√®s.")
      } else {
        cli::cli_alert_danger("Il y a eu une erreur lors de la mise √† jour des tags de la table! üòÖ")
        cli::cli_alert_danger("Veuillez contacter votre ing√©nieur de donn√©es.")
        invisible(FALSE)
      }
    }

    if (change_desc) {
      cli::cli_alert_info("Mise √† jour de la description de la table en cours...")
      r2 <- update_glue_table_desc(creds, schema, table, new_table_desc)
      if (r2) {
        cli::cli_alert_success("La description de la table a √©t√© mise √† jour avec succ√®s.")
      } else {
        cli::cli_alert_danger("Il y a eu une erreur lors de la mise √† jour de la description de la table! üòÖ")
        cli::cli_alert_danger("Veuillez contacter votre ing√©nieur de donn√©es.")
        invisible(FALSE)
      }
    }
  }

  invisible(TRUE)
}


#' Traiter les donn√©es en attente d'√™tre ins√©r√©es dans une table
#'
#' @param con Un objet de connexion tel qu'obtenu via `tube::ellipse_connect()`.
#' @param table Le nom de la table qui doit trait√©e
#'
#' @returns TRUE si les donn√©es en attente d'√™tre ins√©r√©es ont bien √©t√© trait√©es.
#' FALSE sinon.
#' @export
ellipse_process <- function(con, table) {
  env <- DBI::dbGetInfo(con)$profile_name
  schema <- DBI::dbGetInfo(con)$dbms.name
  creds <- get_aws_credentials(env)

  if (!check_params_before_refresh(con, schema, table)) {
    invisible(FALSE)
  }

  # confirm by the user
  if (!ask_yes_no("√ätes-vous certain.e de vouloir rafra√Æchir la table?")) {
    cli::cli_alert_info("Rafra√Æchissement de la table abandonn√©.")
    invisible(FALSE)
  }

  cli::cli_alert_info("Rafra√Æchissement de la table en cours...")
  glue_job <- list_glue_jobs(creds)

  database <- dplyr::case_when(
    grepl("datawarehouse", schema) ~ "datawarehouse",
    grepl("datamart", schema) ~ "datamarts",
    TRUE ~ NA
  )

  if (is.na(database)) {
    cli::cli_alert_danger("Oups, il semble que la base de donn√©es n'a pas √©t√© trouv√©e! üòÖ")
    invisible(FALSE)
  }

  logger::log_debug(paste(
    "[ellipse_process] about to run glue job on database = ",
    database, " schema = ", schema, " table = ", table
  ))

  r <- run_glue_job(creds, glue_job, database, table, NULL, NULL)

  if (r) {
    if (r == -1) {
      cli::cli_alert_info("Il n'y a aucune nouvelle donn√©e √† traiter.")
    } else {
      cli::cli_alert_success("Le traitement des donn√©es a √©t√© d√©clench√© avec succ√®s.")
      cli::cli_alert_info("Les donn√©es seront disponibles dans les prochaines minutes\n")
      cli::cli_alert_info(
        "N'oubliez pas de vous d√©connecter de la plateforme ellipse avec `ellipse_disconnect(...)` üëã."
      )
    }
    invisible(TRUE)
  } else {
    cli::cli_alert_danger("Il y a eu une erreur lors du rafra√Æchissement de la table! üòÖ")
    cli::cli_alert_danger("Veuillez contacter votre ing√©nieur de donn√©es.")
    invisible(FALSE)
  }
}
